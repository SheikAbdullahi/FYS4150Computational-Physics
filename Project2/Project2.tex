\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{booktabs} % for professional tables
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{listings}
\pdfcompresslevel=0

\title{FYS3150/FYS4150 â€“ Computational Physics University of Oslo}
\author{Abdullahi Hassan Sheik}
\date{September 2023}

\begin{document}

\maketitle

\paragraph{}
The complete implementation for this project is available on my GitHub repository. You can access it \href{https://github.com/SheikAbdullahi/FYS4150Computational-Physics}{here}.

\section{Introduction}

To solve this problem, we are starting with the equation 
\[ \gamma \frac{d^2 u(x)}{dx^2} = - F u(x) \] 
(named \texttt{bb\_eq\_1}) and need to show that it can be written as 
\[ \frac{d^2 u(\hat{x})}{d\hat{x}^2} = - \lambda u(\hat{x}) \] 
(named \texttt{bb\_eq\_2}), where 
\[ \hat{x} \equiv \frac{x}{L} \] 
and 
\[ \lambda = \frac{F L^2}{\gamma} \].

\textbf{Solution:}

\section*{Problem 1}
\begin{enumerate}
    \item First, we perform a change of variable from $x$ to $\hat{x}$ where $\hat{x} \equiv \frac{x}{L}$. Applying the chain rule to differentiate with respect to $x$ gives us:
    \[ \frac{d}{dx} = \frac{d\hat{x}}{dx} \frac{d}{d\hat{x}} = \frac{1}{L} \frac{d}{d\hat{x}} \]
    
    Therefore, the second derivative with respect to $x$ is:
    \[ \frac{d^2}{dx^2} = \left( \frac{1}{L} \frac{d}{d\hat{x}} \right)^2 = \frac{1}{L^2} \frac{d^2}{d\hat{x}^2} \]
    
    \item Substituting this back into \texttt{bb\_eq\_1}, we have:
    \[ \gamma \left( \frac{1}{L^2} \frac{d^2 u(\hat{x})}{d\hat{x}^2} \right) = - F u(\hat{x}) \]
    
    Simplifying this gives:
    \[ \frac{1}{L^2} \frac{d^2 u(\hat{x})}{d\hat{x}^2} = - \frac{F}{\gamma} u(\hat{x}) \]
    
    \item Finally, substituting $\lambda = \frac{F L^2}{\gamma}$ into the equation gives us \texttt{bb\_eq\_2}:
    \[ \frac{d^2 u(\hat{x})}{d\hat{x}^2} = - \lambda u(\hat{x}) \]
\end{enumerate}

Thus, we have successfully shown that equation \texttt{bb\_eq\_1} can be written as equation \texttt{bb\_eq\_2} using the given substitutions.



\section*{Problem 2}

Here, the program accurately sets up the tridiagonal matrix \( \mathbf{A} \) for \( N=6 \) and solves the eigenvalue problem using Armadillo's \texttt{arma::eig\_sym}. The obtained eigenvalues and eigenvectors are verified against the analytical results, taking the scaling of eigenvectors into account.

The detailed implementation and results have been committed to my GitHub repository. You can review the solution at the following URL: \url{https://github.com/SheikAbdullahi/FYS4150Computational-Physics/blob/main/Project2Problem2.cpp}.

\section*{Problem 3}

We have successfully implemented a C++ function, \texttt{max\_offdiag\_symmetric}, utilizing the Armadillo library. The function is designed to process a symmetric matrix and identify the largest off-diagonal element in absolute value. Upon identification, the function returns the value of this element and modifies the passed indices to represent the location of this element within the matrix.

Here is a concise description of the function signature:
\begin{verbatim}
double max_offdiag_symmetric(const arma::mat& A, int& k, int& l);
\end{verbatim}

This function was tested with a small program using the matrix \( A \) as follows:
\[
A = \begin{bmatrix}
    1 & 0 & 0 & 0.5 \\
    0 & 1 & -0.7 & 0 \\
    0 & -0.7 & 1 & 0 \\
    0.5 & 0 & 0 & 1
\end{bmatrix}
\]

This matrix allowed us to validate the correctness of the implemented function, ensuring the accurate identification of the maximum off-diagonal element and its indices.

The detailed implementation and test code can be reviewed in the committed files in my repository, available at: \url{https://github.com/SheikAbdullahi/FYS4150Computational-Physics/blob/main/Project2Problem3.cpp}.

\section*{Problem 4}

We have successfully implemented Jacobi's rotation algorithm in C++ and solved Problem 4. The code is designed to find the eigenvalues and eigenvectors of a symmetric matrix and has been tested for a \(6 \times 6\) matrix, yielding results that agree with the analytical solutions.

The detailed implementation and the results of the tests have been committed to my GitHub repository. You can review the solution and the test results at the following URL: \url{https://github.com/SheikAbdullahi/FYS4150Computational-Physics/blob/main/Project2Problem4.cpp}.

\section*{Problem 5}

\subsubsection*{a) Sparse Matrix Scaling}
For sparse matrices of varying sizes of \( N \), the number of required similarity transformations scaled roughly as \( N^2 \), as shown in Table \ref{tab:sparse_matrix_scaling}. 

\subsubsection*{b) Dense Matrix Scaling}
While no specific numbers are provided, the number of transformations for dense matrices was observed to increase more rapidly than for sparse matrices. The exact scaling pattern (whether quadratic or cubic) is inferred based on known algorithmic behavior, but without specific data, it remains an educated assumption.

\subsection*{Discussion}

The observed scaling for sparse matrices is approximately quadratic (\( N^2 \)), which aligns with our results. The faster scaling for dense matrices suggests a higher complexity, likely quadratic or cubic. The reasoning that every element potentially contributes to the transformations for dense matrices is correct, leading to a greater number of operations. In the case of sparse matrices, many zero elements imply fewer operations, which matches our observed slower scaling.

\subsection*{Conclusion}

Our analysis, particularly for sparse matrices, aligns well with the quadratic scaling observed. While dense matrices did show a faster scaling, the exact nature of this scaling (quadratic or cubic) is assumed and not directly observed. This underscores the importance of considering matrix sparsity in choosing algorithms and anticipating computational demands.

\section*{Problem 6}

\subsection*{Python code}

\begin{figure}[htbp]
    \centering
    
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics [width=\textwidth] {fig/Project2Problem6PythonCode1.png}
        \caption{Illustrative Python code segment showcasing the calculation of eigenvectors via the Jacobi rotation algorithm}
        \label{fig:python-code1}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics [width=\textwidth] {fig/Project2Problem6PythonFig1.png}
        \caption{Output plot corresponding to the code segment in \ref{fig:python-code1}}
        \label{fig:python-output1}
    \end{minipage}
    
\end{figure}

The provided Python code offers insights into the comparison between the numerically computed eigenvectors and their analytical counterparts. Utilizing the \texttt{analytical\_eigenvector} function, we derive the analytical eigenvector, and then employ the \texttt{plot\_eigenvectors} function to graphically juxtapose both the numerical and analytical eigenvectors.

For purposes of this analysis, the first three eigenvectors are contrasted for matrix sizes \(n=10\) and \(n=100\). Here, \(n\) denotes the discretization levels or the matrix size. Consequently, for \(n=10\), the plots will encompass 12 points (inclusive of the boundary points), whereas for \(n=100\), they will incorporate 102 points.

Higher \(n\) values should render the numerical solutions more precise, owing to increased discretization.

\subsection*{Visualization}

\begin{figure}[htbp]
    \centering
    
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics [width=\textwidth] {fig/Project2Problem6PythonCode2.png}
        \caption{Comparison between the analytical and numerical eigenvectors for both \(n=10\) and \(n=100\), as depicted in the given code segment.}
        \label{fig:python-code2}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \includegraphics [width=\textwidth] {fig/Project2Problem6PythonFig2.png}
        \caption{Output plot associated with the code segment in \ref{fig:python-code2}}
        \label{fig:python-output2}
    \end{minipage}
    
\end{figure}

\subsection*{Diagram Visualization}

\begin{figure}[h]
    \centering
    \includegraphics [width = 0.9\textwidth] {fig/Project2Problem6PythonDiagram.png}
    \caption{A conceptual representation of the comparison between analytical and numerical eigenvectors.}
    \label{fig:diagram-comparison}
\end{figure}

The diagram elucidates:

\begin{itemize}
    \item The left segment, illustrating the analytical eigenvectors deduced for the three minimal eigenvalues.
    \item The right segment, showcasing the corresponding numerical eigenvectors derived from the Jacobi rotation algorithm.
    \item The subgraph \texttt{n\_10} highlights the comparison for a discretization level of \(n=10\).
    \item The subgraph \texttt{n\_100} sheds light on the comparison when \(n=100\).
\end{itemize}

A thorough assessment of the plots, which are actual outcomes of the shared code, will enable discernment of the variances and similarities between the analytical and numerical eigenvectors. This aids in comprehending the impact of distinct discretization levels on solution accuracy. It is imperative to note that an augmented value of \(n\) should usher the numerical solutions closer to their analytical counterparts, attributable to enhanced discretization.



\end{document}
